{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Multilayer Perceptron Algorithm Feedforward and Backpropagation Passes\n",
    "======================================================================\n",
    "\n",
    "This demonstrates an implementation of a neural network using computational graphs and matrix operations\n",
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Joseph Emmanuel Dayo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the dilpack package containing the main source code can be reference make sure to do:\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "on the project directory first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dilpack.learn.computational.nodes import Node, MatMul, Add, Sigmoid, Tanh, LeakyRelu\n",
    "from dilpack.learn.computational.backpropagate import backpropagate, update\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Node(np.array([[0.4, 0.5, -0.7]]))\n",
    "\n",
    "learning_rate = 0.85\n",
    "biases_layer1 = Node(np.array([[-0.7, -0.8, -0.9, -0.4]]), trainable=True)\n",
    "weights_layer1 = Node(np.transpose(np.array([[0.1, 0.4, -0.6],\n",
    "                           [0.3, 0.7, -0.4],\n",
    "                           [-0.9, 0.3, -0.6],\n",
    "                           [-0.8, 0.7, 0.2]])), trainable=True)\n",
    "\n",
    "biases_layer2 = Node(np.array([[-0.7, -0.6, 0.4]]), trainable=True)\n",
    "\n",
    "weights_layer2 = Node(np.transpose(np.array([[-0.3 ,0.8 ,-0.9 ,0.4],\n",
    "                           [-0.4 ,-0.2 ,0.7 ,-0.3],\n",
    "                           [0.4,-0.7,0.8, -0.3]])), trainable=True)\n",
    "\n",
    "biases_output = Node(np.array([[-0.6,-0.4]]), trainable=True)\n",
    "weights_output = Node(np.transpose(np.array([[0.8, -0.3, 0.7],\n",
    "                        [-0.6, 0.7, 0.3]])), trainable=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the computational graph for the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer1 = LeakyRelu(Add(MatMul(input, weights_layer1), biases_layer1))\n",
    "output_layer2 = LeakyRelu(Add(MatMul(output_layer1, weights_layer2), biases_layer2))\n",
    "output = Sigmoid(Add(MatMul(output_layer2, weights_output), biases_output), logistic_slope=0.7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the structure of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SIGMOID(logistic-slope: 0.7)\n",
      "   input 1:\n",
      "    Add(A,B)\n",
      "      input 1:\n",
      "        MatMul(A,B)\n",
      "          input 1:\n",
      "            LeakyRelu(A,slope=0.01)\n",
      "              input 1:\n",
      "                Add(A,B)\n",
      "                  input 1:\n",
      "                    MatMul(A,B)\n",
      "                      input 1:\n",
      "                        LeakyRelu(A,slope=0.01)\n",
      "                          input 1:\n",
      "                            Add(A,B)\n",
      "                              input 1:\n",
      "                                MatMul(A,B)\n",
      "                                  input 1:\n",
      "                                    Value(Trainable=False)\n",
      "                                      input 1:\n",
      "                                      [[ 0.4  0.5 -0.7]]\n",
      "                                  input 2:\n",
      "                                    Value(Trainable=True)\n",
      "                                      input 1:\n",
      "                                      [[ 0.1  0.3 -0.9 -0.8]\n",
      " [ 0.4  0.7  0.3  0.7]\n",
      " [-0.6 -0.4 -0.6  0.2]]\n",
      "                              input 2:\n",
      "                                Value(Trainable=True)\n",
      "                                  input 1:\n",
      "                                  [[-0.7 -0.8 -0.9 -0.4]]\n",
      "                      input 2:\n",
      "                        Value(Trainable=True)\n",
      "                          input 1:\n",
      "                          [[-0.3 -0.4  0.4]\n",
      " [ 0.8 -0.2 -0.7]\n",
      " [-0.9  0.7  0.8]\n",
      " [ 0.4 -0.3 -0.3]]\n",
      "                  input 2:\n",
      "                    Value(Trainable=True)\n",
      "                      input 1:\n",
      "                      [[-0.7 -0.6  0.4]]\n",
      "          input 2:\n",
      "            Value(Trainable=True)\n",
      "              input 1:\n",
      "              [[ 0.8 -0.6]\n",
      " [-0.3  0.7]\n",
      " [ 0.7  0.3]]\n",
      "      input 2:\n",
      "        Value(Trainable=True)\n",
      "          input 1:\n",
      "          [[-0.6 -0.4]]\n"
     ]
    }
   ],
   "source": [
    "output.dump()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform feedforward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44312378 0.45095094]]\n"
     ]
    }
   ],
   "source": [
    "predicted_values = output.forward()\n",
    "print(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = np.array([[0.83,0.74]])\n",
    "error = (expected_output - predicted_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to print the resulting gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06682728 0.05009681]]\n",
      "[array([[ 0.79960459, -0.60029642],\n",
      "       [-0.30034255,  0.69974321],\n",
      "       [ 0.72250542,  0.3168711 ]])]\n",
      "[array([[-0.54319681, -0.35741771]])]\n",
      "[array([[ 0.10008362,  0.29985343, -0.89983224, -0.80006288],\n",
      "       [ 0.40010452,  0.69981679,  0.3002097 ,  0.6999214 ],\n",
      "       [-0.60014633, -0.3997435 , -0.60029358,  0.20011004]])]\n"
     ]
    }
   ],
   "source": [
    "backpropagate(output, error)\n",
    "update(output, learning_rate=learning_rate)\n",
    "\n",
    "# we can print the computed gradients at each node.\n",
    "\n",
    "print(output.errors[0])\n",
    "print(weights_output.input)\n",
    "print(biases_output.input)\n",
    "print(weights_layer1.input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample machine learning using a real dataset\n",
    "============================================"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the iris test data set. Objective is to classify Iris species using supplied features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "train_set_labels = []\n",
    "validation_set = []\n",
    "validation_set_labels = []\n",
    "\n",
    "label_encodings = {\n",
    "  \"Iris-setosa\": [1., 0., 0.],\n",
    "  \"Iris-versicolor\": [0., 1., 0.],\n",
    "  \"Iris-virginica\": [0., 0., 1.],\n",
    "}\n",
    "\n",
    "with open(os.path.join('datasets', 'iris.data'),\"r\") as file:\n",
    "  csv_rows = [x.strip().split(',') for x in file.readlines()]\n",
    "  total_samples = len(csv_rows)\n",
    "  test_set_indexes = random.sample(range(total_samples), k=int(total_samples * .3))\n",
    "  for idx, sample in enumerate(csv_rows):\n",
    "    row_values = [float(x) for x in sample[0:4]]\n",
    "    row_label = sample[4]\n",
    "    if idx in test_set_indexes:\n",
    "      validation_set.append(row_values)\n",
    "      validation_set_labels.append(label_encodings.get(row_label))\n",
    "    else:\n",
    "      train_set.append(row_values)\n",
    "      train_set_labels.append(label_encodings.get(row_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model with 2 hidden layers and one output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Node(np.array([[5.1,3.5,1.4,0.2]]))\n",
    "learning_rate = 0.01\n",
    "\n",
    "# set number of nodes per layer\n",
    "NUM_HIDDEN_LAYER1 = 3\n",
    "NUM_HIDDEN_LAYER2 = 2\n",
    "NUM_OUTPUT_LAYER = 3\n",
    "\n",
    "biases_layer1 = Node(np.random.uniform(size= [1,NUM_HIDDEN_LAYER1]), trainable=True)\n",
    "weights_layer1 = Node(np.random.uniform(size=[4,NUM_HIDDEN_LAYER1]), trainable=True)\n",
    "biases_layer2 = Node(np.random.uniform(size=[1,NUM_HIDDEN_LAYER2]), trainable=True)\n",
    "weights_layer2 = Node(np.random.uniform(size=[NUM_HIDDEN_LAYER1,NUM_HIDDEN_LAYER2]), trainable=True)\n",
    "biases_output = Node(np.random.uniform(size=[1,NUM_OUTPUT_LAYER]), trainable=True)\n",
    "weights_output = Node(np.random.uniform(size=[NUM_HIDDEN_LAYER2,NUM_OUTPUT_LAYER]), trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer1 = Tanh(Add(MatMul(input, weights_layer1), biases_layer1))\n",
    "output_layer2 = Tanh(Add(MatMul(output_layer1, weights_layer2), biases_layer2))\n",
    "output = Sigmoid(Add(MatMul(output_layer2, weights_output), biases_output), logistic_slope=0.7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network on the training set and evaluate the mean square rror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 best=0.5648634885660312 error=0.5648634885660312\n",
      "epoch: 10 best=0.3568732435085851 error=0.3568732435085851\n",
      "epoch: 20 best=0.33599141554164486 error=0.33599141554164486\n",
      "epoch: 30 best=0.3334450216626845 error=0.3334450216626845\n",
      "epoch: 40 best=0.33307926838968854 error=0.33307926838968854\n",
      "epoch: 50 best=0.3330076549065788 error=0.3330076549065788\n",
      "epoch: 60 best=0.3329866561888947 error=0.3329866561888947\n",
      "epoch: 70 best=0.33297848598406576 error=0.33297848598406576\n",
      "epoch: 80 best=0.3329759593529792 error=0.3329759593529792\n",
      "epoch: 90 best=0.33297593585513563 error=0.3329773649610218\n",
      "epoch: 100 best=0.33297593585513563 error=0.33298219952363434\n",
      "epoch: 110 best=0.33297593585513563 error=0.3329902845032541\n",
      "epoch: 120 best=0.33297593585513563 error=0.3330015410768016\n",
      "epoch: 130 best=0.33297593585513563 error=0.333015920010432\n",
      "epoch: 140 best=0.33297593585513563 error=0.3330333723650572\n",
      "epoch: 150 best=0.33297593585513563 error=0.33305383020054874\n",
      "epoch: 160 best=0.33297593585513563 error=0.33307718847938617\n",
      "epoch: 170 best=0.33297593585513563 error=0.3331032863657977\n",
      "epoch: 180 best=0.33297593585513563 error=0.3331318886984188\n",
      "epoch: 190 best=0.33297593585513563 error=0.3331626694950264\n",
      "epoch: 200 best=0.33297593585513563 error=0.33319519978615425\n",
      "epoch: 210 best=0.33297593585513563 error=0.3332289420965456\n",
      "epoch: 220 best=0.33297593585513563 error=0.33326325353299346\n",
      "epoch: 230 best=0.33297593585513563 error=0.3332973987034086\n",
      "epoch: 240 best=0.33297593585513563 error=0.3333305726436387\n",
      "epoch: 250 best=0.33297593585513563 error=0.33336193269919173\n",
      "epoch: 260 best=0.33297593585513563 error=0.3333906370955576\n",
      "epoch: 270 best=0.33297593585513563 error=0.33341588695417274\n",
      "epoch: 280 best=0.33297593585513563 error=0.3334369679621404\n",
      "epoch: 290 best=0.33297593585513563 error=0.33345328789355655\n",
      "epoch: 300 best=0.33297593585513563 error=0.33346440670988703\n",
      "epoch: 310 best=0.33297593585513563 error=0.33347005692755377\n",
      "epoch: 320 best=0.33297593585513563 error=0.3334701531430521\n",
      "epoch: 330 best=0.33297593585513563 error=0.3334647908277111\n",
      "epoch: 340 best=0.33297593585513563 error=0.3334542355448986\n",
      "epoch: 350 best=0.33297593585513563 error=0.3334389044650246\n",
      "epoch: 360 best=0.33297593585513563 error=0.33341934240515286\n",
      "epoch: 370 best=0.33297593585513563 error=0.333396194627875\n",
      "epoch: 380 best=0.33297593585513563 error=0.33337017838345173\n",
      "epoch: 390 best=0.33297593585513563 error=0.3333420547817173\n",
      "epoch: 400 best=0.33297593585513563 error=0.3333126021444408\n",
      "epoch: 410 best=0.33297593585513563 error=0.33328259159906914\n",
      "epoch: 420 best=0.33297593585513563 error=0.3332527653802479\n",
      "epoch: 430 best=0.33297593585513563 error=0.33322381811860785\n",
      "epoch: 440 best=0.33297593585513563 error=0.3331963812975801\n",
      "epoch: 450 best=0.33297593585513563 error=0.33317101100710256\n",
      "epoch: 460 best=0.33297593585513563 error=0.33314817906841226\n",
      "epoch: 470 best=0.33297593585513563 error=0.33312826750494995\n",
      "epoch: 480 best=0.33297593585513563 error=0.33311156617271787\n",
      "epoch: 490 best=0.33297593585513563 error=0.33309827315518564\n",
      "epoch: 500 best=0.33297593585513563 error=0.3330884973222564\n",
      "epoch: 510 best=0.33297593585513563 error=0.3330822623172252\n",
      "epoch: 520 best=0.33297593585513563 error=0.33307951122937174\n",
      "epoch: 530 best=0.33297593585513563 error=0.3330801113570768\n",
      "epoch: 540 best=0.33297593585513563 error=0.333083858741568\n",
      "epoch: 550 best=0.33297593585513563 error=0.3330904824854954\n",
      "epoch: 560 best=0.33297593585513563 error=0.33309964917608365\n",
      "epoch: 570 best=0.33297593585513563 error=0.3331109679359612\n",
      "epoch: 580 best=0.33297593585513563 error=0.3331239966896158\n",
      "epoch: 590 best=0.33297593585513563 error=0.3331382501664248\n",
      "epoch: 600 best=0.33297593585513563 error=0.3331532100000342\n",
      "epoch: 610 best=0.33297593585513563 error=0.3331683370768455\n",
      "epoch: 620 best=0.33297593585513563 error=0.3331830860737236\n",
      "epoch: 630 best=0.33297593585513563 error=0.3331969219286071\n",
      "epoch: 640 best=0.33297593585513563 error=0.33320933781034157\n",
      "epoch: 650 best=0.33297593585513563 error=0.3332198739861504\n",
      "epoch: 660 best=0.33297593585513563 error=0.33322813681446\n",
      "epoch: 670 best=0.33297593585513563 error=0.33323381691260306\n",
      "epoch: 680 best=0.33297593585513563 error=0.33323670537429\n",
      "epoch: 690 best=0.33297593585513563 error=0.33323670677193945\n",
      "epoch: 700 best=0.33297593585513563 error=0.3332338476241841\n",
      "epoch: 710 best=0.33297593585513563 error=0.33322827909942415\n",
      "epoch: 720 best=0.33297593585513563 error=0.33322027301571694\n",
      "epoch: 730 best=0.33297593585513563 error=0.33321021070975204\n",
      "epoch: 740 best=0.33297593585513563 error=0.3331985650562837\n",
      "epoch: 750 best=0.33297593585513563 error=0.3331858767345736\n",
      "epoch: 760 best=0.33297593585513563 error=0.33317272661550956\n",
      "epoch: 770 best=0.33297593585513563 error=0.3331597067128897\n",
      "epoch: 780 best=0.33297593585513563 error=0.33314739235810853\n",
      "epoch: 790 best=0.33297593585513563 error=0.3331363180431065\n",
      "epoch: 800 best=0.33297593585513563 error=0.3331269587566058\n",
      "epoch: 810 best=0.33297593585513563 error=0.3331197177370794\n",
      "epoch: 820 best=0.33297593585513563 error=0.3331149205707757\n",
      "epoch: 830 best=0.33297593585513563 error=0.33311281467306125\n",
      "epoch: 840 best=0.33297593585513563 error=0.3331135725627928\n",
      "epoch: 850 best=0.33297593585513563 error=0.3331172970538935\n",
      "epoch: 860 best=0.33297593585513563 error=0.33312402654766227\n",
      "epoch: 870 best=0.33297593585513563 error=0.33313373895414694\n",
      "epoch: 880 best=0.33297593585513563 error=0.3331463533096209\n",
      "epoch: 890 best=0.33297593585513563 error=0.33316172879256434\n",
      "epoch: 900 best=0.33297593585513563 error=0.33317966148533784\n",
      "epoch: 910 best=0.33297593585513563 error=0.33319987980971566\n",
      "epoch: 920 best=0.33297593585513563 error=0.33322204001914646\n",
      "epoch: 930 best=0.33297593585513563 error=0.3332457234038588\n",
      "epoch: 940 best=0.33297593585513563 error=0.33327043691051905\n",
      "epoch: 950 best=0.33297593585513563 error=0.3332956186674426\n",
      "epoch: 960 best=0.33297593585513563 error=0.33332064944118955\n",
      "epoch: 970 best=0.33297593585513563 error=0.3333448703736461\n",
      "epoch: 980 best=0.33297593585513563 error=0.3333676065463279\n",
      "epoch: 990 best=0.33297593585513563 error=0.33338819510875056\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "best_error_rate = 1.0\n",
    "for i  in range(epochs):\n",
    "  for idx, sample in enumerate(train_set):\n",
    "    input.update([sample])\n",
    "    result = output.forward()\n",
    "    error = [train_set_labels[idx]] - result\n",
    "    backpropagate(output, error)\n",
    "    update(output, learning_rate=learning_rate)\n",
    "  \n",
    "  #evaluate compute for sse\n",
    "  error_rate = 0.\n",
    "  for idx, test in enumerate(validation_set):\n",
    "    input.update([test])\n",
    "    result = output.forward()\n",
    "    error_rate += np.sum(np.power([validation_set_labels[idx]] - result,2)) / 2\n",
    "  error_rate /= len(validation_set)\n",
    "\n",
    "  if (error_rate < best_error_rate):\n",
    "    best_error_rate = error_rate\n",
    "  if i%10==0:\n",
    "    print(f\"epoch: {i} best={best_error_rate} error={error_rate}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2eaf81b38e7365e67bc900d18c2f74bf4ae8018e04c282c650008198a855644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
